{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-Relation-Extractor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "269129c7a40c4f6e9f9def4a35caf940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_420d032cef094bc9b88cf9d298f44d22",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_43b968cb6ec54b758349a34063b03afa",
              "IPY_MODEL_ff29fa6eb4f44d68a62b7e3aa66ea7bd"
            ]
          }
        },
        "420d032cef094bc9b88cf9d298f44d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43b968cb6ec54b758349a34063b03afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_564c1743c73945d6a3e2ca796ef28f63",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9db23025980c436b94f1b8f81a38816d"
          }
        },
        "ff29fa6eb4f44d68a62b7e3aa66ea7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a18e81faa1b541c5874f08939dc51261",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:16&lt;00:00, 29.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7d2b15048ec4f9daa44d1733b607bb9"
          }
        },
        "564c1743c73945d6a3e2ca796ef28f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9db23025980c436b94f1b8f81a38816d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a18e81faa1b541c5874f08939dc51261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7d2b15048ec4f9daa44d1733b607bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdc66f4225d348b0936b723f3a369294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6d0e9f30a12a40d6afac94afbf828954",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c740c9cfc6ba4eb8b7548df50869f9cf",
              "IPY_MODEL_b622e9bfef6b49e4b9dd53e1d330fc62"
            ]
          }
        },
        "6d0e9f30a12a40d6afac94afbf828954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c740c9cfc6ba4eb8b7548df50869f9cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba1eeff95bf141489ae0fe640ce78391",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_303c8be56e184c6a9e6526636d3c3aac"
          }
        },
        "b622e9bfef6b49e4b9dd53e1d330fc62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a1d0a2761b84bacafd3a1ace881b929",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:15&lt;00:00, 32.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_482b4b3d214e49f5b7404f298262962f"
          }
        },
        "ba1eeff95bf141489ae0fe640ce78391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "303c8be56e184c6a9e6526636d3c3aac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a1d0a2761b84bacafd3a1ace881b929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "482b4b3d214e49f5b7404f298262962f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDkoMDM21M-R",
        "outputId": "064fa98c-4322-4898-89f2-e652de19f76c"
      },
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install transformers\n",
        "!pip install torch torchvision\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX0Qhns01_LR"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4ICjFN_2Cr4"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification, AdamW, BertForSequenceClassification\n",
        "from transformers import DistilBertForTokenClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import statistics\n",
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-tkvP2J414E"
      },
      "source": [
        "Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxGEdhrH43nP"
      },
      "source": [
        "def read_data_entity_marker(tags):\n",
        "    context_text = []\n",
        "    context_tags = []\n",
        "\n",
        "    for tagg in tags:\n",
        "        context_text.append(tagg[1])\n",
        "    return context_text\n",
        "\n",
        "\n",
        "def read_data_tag_sentence(texts, tags):\n",
        "    context_text = []\n",
        "    context_tags = []\n",
        "\n",
        "    for textt in texts:\n",
        "        for tagg in tags:\n",
        "            if int(textt[0]) == int(tagg[0]):\n",
        "                context_text.append(textt[1])\n",
        "                context_tags.append(tagg[1])\n",
        "            else:\n",
        "                continue\n",
        "    return context_text, context_tags\n",
        "\n",
        "\n",
        "def read_labels(dataa):\n",
        "    labels = []\n",
        "    count = 0\n",
        "    for dat in dataa:\n",
        "        for ent in dat[\"entities\"]:\n",
        "            labels.append(int(ent[\"label\"]))\n",
        "            count += 1\n",
        "    return labels"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2pGiXUk482z"
      },
      "source": [
        "Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71aAKBS35GoT"
      },
      "source": [
        "def tokenizeData_entity_marker(tokenizer, text, max_length=256):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for tx in text:\n",
        "        tokenizedData = tokenizer.encode_plus(tx, max_length=max_length,\n",
        "                                              padding='max_length', truncation=\"longest_first\")\n",
        "        tokenizedQP = tokenizedData[\"input_ids\"]\n",
        "        attentionMask = tokenizedData[\"attention_mask\"]\n",
        "\n",
        "        input_ids.append(tokenizedQP)\n",
        "        attention_masks.append(attentionMask)\n",
        "\n",
        "    return np.array(input_ids), np.array(attention_masks)\n",
        "\n",
        "def tokenizeData_tag_sentence(tokenizer, text, tags, max_length=256):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for tx, tg in zip(text, tags):\n",
        "        tokenizedData = tokenizer.encode_plus(tx, tg, max_length=max_length,\n",
        "                                              padding='max_length', truncation=\"longest_first\")\n",
        "        tokenizedQP = tokenizedData[\"input_ids\"]\n",
        "        attentionMask = tokenizedData[\"attention_mask\"]\n",
        "\n",
        "        input_ids.append(tokenizedQP)\n",
        "        attention_masks.append(attentionMask)\n",
        "\n",
        "    return np.array(input_ids), np.array(attention_masks)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCUC-qBz5LbW"
      },
      "source": [
        "Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrw3DsHi5M7-"
      },
      "source": [
        "def buildDataLoaders(batchSize, trainFeatures, testFeatures):\n",
        "    trainTensors = [torch.tensor(feature, dtype=torch.long) for feature in trainFeatures]\n",
        "    testTensors = [torch.tensor(feature, dtype=torch.long) for feature in testFeatures]\n",
        "\n",
        "    trainDataset = TensorDataset(*trainTensors)\n",
        "    testDataset = TensorDataset(*testTensors)\n",
        "\n",
        "    trainSampler = RandomSampler(trainDataset)\n",
        "    testSampler = SequentialSampler(testDataset)\n",
        "\n",
        "    trainDataloader = DataLoader(trainDataset, sampler=trainSampler, batch_size=batchSize)\n",
        "    testDataloader = DataLoader(testDataset, sampler=testSampler, batch_size=batchSize)\n",
        "\n",
        "    return trainDataloader, testDataloader"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7bhFaRg5V1x"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWzS1DWN5XAv"
      },
      "source": [
        "def train(numEpochs, gradSteps, model, optimizer, trainDataLoader):\n",
        "    trainLossHistory = []\n",
        "\n",
        "    for _ in tqdm(range(numEpochs), desc=\"Training Epoch's\"):\n",
        "\n",
        "        # Train the model for fine-tuning\n",
        "        epochTrainLoss = 0  # Cumulative loss\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "\n",
        "        for step, batch in enumerate(trainDataLoader):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_masks = batch[1].to(device)\n",
        "            label = batch[2].to(device)\n",
        "            outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=label)\n",
        "\n",
        "            loss = outputs[0]\n",
        "            loss = loss / gradSteps\n",
        "            epochTrainLoss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % gradSteps == 0:  # Gradient accumulation is over\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clipping gradients\n",
        "                optimizer.step()\n",
        "                model.zero_grad()\n",
        "\n",
        "        epochTrainLoss = epochTrainLoss / len(trainDataLoader)\n",
        "        trainLossHistory.append(epochTrainLoss)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oDotOtQ5aze"
      },
      "source": [
        "Predict and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPfE_QKd5fgm"
      },
      "source": [
        "def predict_tag_sentence(tokenizer, model, text, tag, max_length=256):\n",
        "    sequence = tokenizer.encode_plus(tag, text, max_length=max_length,\n",
        "                                     padding='max_length', truncation=\"longest_first\"\n",
        "                                     , return_tensors=\"pt\")['input_ids'].to(device)\n",
        "\n",
        "    logits = model(sequence)[0]\n",
        "    probabilities = torch.softmax(logits, dim=1).detach().cpu().tolist()[0]\n",
        "    if probabilities[1] > 0.5:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def predict_entity_marker(tokenizer, model, text, max_length=256):\n",
        "    sequence = tokenizer.encode_plus(text, max_length=max_length,\n",
        "                                     padding='max_length', truncation=\"longest_first\"\n",
        "                                     , return_tensors=\"pt\")['input_ids'].to(device)\n",
        "\n",
        "    logits = model(sequence)[0]\n",
        "    probabilities = torch.softmax(logits, dim=1).detach().cpu().tolist()[0]\n",
        "    if probabilities[1] > 0.5:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def evaluate(pred_labels, test_labels):\n",
        "    pred_labels1 = np.array_split(pred_labels, 5)\n",
        "    test_labels1 = np.array_split(test_labels, 5)\n",
        "    accuracy = []\n",
        "    f1 = []\n",
        "    for test, pred in zip(test_labels1, pred_labels1):\n",
        "        accuracy.append(accuracy_score(test, pred))\n",
        "        f1.append(f1_score(test, pred, average=\"weighted\"))\n",
        "\n",
        "    print(\"Accuracy: \" + str(sum(accuracy) / len(accuracy)))\n",
        "    print(\"Standard Deviation: \" + str(statistics.stdev(accuracy)))\n",
        "\n",
        "    print(\"F1 Score: \" + str(sum(f1) / len(f1)))\n",
        "    print(\"Standard Deviation: \" + str(statistics.stdev(f1)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n12RnuT5lG8"
      },
      "source": [
        "BERT Entity Marker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK8dH8Gx5muP"
      },
      "source": [
        "def bert_entity_marker_a(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append(\n",
        "                [d[\"id\"],\n",
        "                 d[\"text\"].replace(ent[\"arg1\"] + \" \", \"[e]\" + ent[\"arg1\"] + \"[\\e]\").replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                            \"[e]\" + ent[\n",
        "                                                                                                \"arg2\"] + \"[\\e]\").strip(\n",
        "                     \"\\n\")])\n",
        "\n",
        "    text = read_data_entity_marker(tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=.2)\n",
        "\n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    train_ids, train_attn = tokenizeData_entity_marker(tokenizer, train_texts)\n",
        "    test_ids, test_attn = tokenizeData_entity_marker(tokenizer, test_texts)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-cased').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text in test_texts:\n",
        "        pred_labels.append(predict_entity_marker(tokenizer, model, text))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)\n",
        "\n",
        "\n",
        "def bert_entity_marker_b(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append(\n",
        "                [d[\"id\"],\n",
        "                 d[\"text\"].replace(ent[\"arg1\"] + \" \", \"[e] \" + ent[\"arg1\"] + \" [\\e]\").replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                            \"[e] \" + ent[\n",
        "                                                                                                \"arg2\"] + \" [\\e]\").strip(\n",
        "                     \"\\n\")])\n",
        "\n",
        "    text = read_data_entity_marker(tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=.2)\n",
        "\n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    train_ids, train_attn = tokenizeData_entity_marker(tokenizer, train_texts)\n",
        "    test_ids, test_attn = tokenizeData_entity_marker(tokenizer, test_texts)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-cased').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text in test_texts:\n",
        "        pred_labels.append(predict_entity_marker(tokenizer, model, text))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)\n",
        "\n",
        "\n",
        "def bert_entity_marker_c(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append(\n",
        "                [d[\"id\"],\n",
        "                 d[\"text\"].replace(ent[\"arg1\"] + \" \", \"ENTITY1\" + ent[\"arg1\"]).replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                            \"ENTITY2\" + ent[\n",
        "                                                                                                \"arg2\"]).strip(\n",
        "                     \"\\n\")])\n",
        "\n",
        "    text = read_data_entity_marker(tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=.2)\n",
        "\n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    train_ids, train_attn = tokenizeData_entity_marker(tokenizer, train_texts)\n",
        "    test_ids, test_attn = tokenizeData_entity_marker(tokenizer, test_texts)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-cased').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text in test_texts:\n",
        "        pred_labels.append(predict_entity_marker(tokenizer, model, text))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MbiMw176RkY"
      },
      "source": [
        "BERT Tag Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9bAcpkQ6TnY"
      },
      "source": [
        "def bert_tag_sentence(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        text_list.append([d[\"id\"], d[\"text\"].strip(\"\\n\")])\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append([d[\"id\"], d[\"text\"].replace(ent[\"arg1\"] + \" \", \"ENTITY1 \").replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                               \" ENTITY2 \").strip(\n",
        "                \"\\n\")])\n",
        "\n",
        "    text, tags = read_data_tag_sentence(text_list, tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_tags, test_tags, train_labels, test_labels = train_test_split(text, tags,\n",
        "                                                                                                 labels, test_size=.2)\n",
        "    \n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    train_ids, train_attn = tokenizeData_tag_sentence(tokenizer, train_texts, train_tags)\n",
        "    test_ids, test_attn = tokenizeData_tag_sentence(tokenizer, test_texts, test_tags)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-cased').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text, tags in zip(test_texts, test_tags):\n",
        "        pred_labels.append(predict_tag_sentence(tokenizer, model, text, tags))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7GP0B466n4w"
      },
      "source": [
        "Roberta Entity Marker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Xg2bF86qTX"
      },
      "source": [
        "def roberta_entity_marker_a(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append(\n",
        "                [d[\"id\"],\n",
        "                 d[\"text\"].replace(ent[\"arg1\"] + \" \", \"[e]\" + ent[\"arg1\"] + \"[\\e]\").replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                            \"[e]\" + ent[\n",
        "                                                                                                \"arg2\"] + \"[\\e]\").strip(\n",
        "                     \"\\n\")])\n",
        "\n",
        "    text = read_data_entity_marker(tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=.2)\n",
        "\n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    train_ids, train_attn = tokenizeData_entity_marker(tokenizer, train_texts)\n",
        "    test_ids, test_attn = tokenizeData_entity_marker(tokenizer, test_texts)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text in test_texts:\n",
        "        pred_labels.append(predict_entity_marker(tokenizer, model, text))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)\n",
        "\n",
        "\n",
        "def roberta_entity_marker_b(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append(\n",
        "                [d[\"id\"],\n",
        "                 d[\"text\"].replace(ent[\"arg1\"] + \" \", \"[e] \" + ent[\"arg1\"] + \" [\\e]\").replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                            \"[e] \" + ent[\n",
        "                                                                                                \"arg2\"] + \" [\\e]\").strip(\n",
        "                     \"\\n\")])\n",
        "\n",
        "    text = read_data_entity_marker(tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=.2)\n",
        "\n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    train_ids, train_attn = tokenizeData_entity_marker(tokenizer, train_texts)\n",
        "    test_ids, test_attn = tokenizeData_entity_marker(tokenizer, test_texts)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = Read.buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text in test_texts:\n",
        "        pred_labels.append(predict_entity_marker(tokenizer, model, text))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)\n",
        "\n",
        "\n",
        "def roberta_entity_marker_c(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append(\n",
        "                [d[\"id\"],\n",
        "                 d[\"text\"].replace(ent[\"arg1\"] + \" \", \"ENTITY1\" + ent[\"arg1\"]).replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                            \"ENTITY2\" + ent[\n",
        "                                                                                                \"arg2\"]).strip(\n",
        "                     \"\\n\")])\n",
        "\n",
        "    text = read_data_entity_marker(tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_labels, test_labels = train_test_split(text, labels, test_size=.2)\n",
        "\n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    train_ids, train_attn = tokenizeData_entity_marker(tokenizer, train_texts)\n",
        "    test_ids, test_attn = tokenizeData_entity_marker(tokenizer, test_texts)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text in test_texts:\n",
        "        pred_labels.append(predict_entity_marker(tokenizer, model, text))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anoHLM5V7Dk6"
      },
      "source": [
        "Roberta Tag Sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55nguhV_7FH7"
      },
      "source": [
        "def roberta_tag_sentence(file):\n",
        "    with open(file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    text_list = []\n",
        "    tag_list = []\n",
        "\n",
        "    for d in data:\n",
        "        text_list.append([d[\"id\"], d[\"text\"].strip(\"\\n\")])\n",
        "        for ent in d[\"entities\"]:\n",
        "            tag_list.append([d[\"id\"], d[\"text\"].replace(ent[\"arg1\"] + \" \", \"ENTITY1 \").replace(\" \" + ent[\"arg2\"] + \" \",\n",
        "                                                                                               \" ENTITY2 \").strip(\n",
        "                \"\\n\")])\n",
        "\n",
        "    text, tags = read_data_tag_sentence(text_list, tag_list)\n",
        "    labels = read_labels(data)\n",
        "\n",
        "    sentences = []\n",
        "    args1 = []\n",
        "    args2 = []\n",
        "\n",
        "    for d in data:\n",
        "      sentences.append([d[\"id\"], d[\"text\"]])\n",
        "      for ent in d[\"entities\"]:\n",
        "        args1.append([d[\"id\"], ent[\"arg1\"]])\n",
        "        args2.append([d[\"id\"], ent[\"arg2\"]])\n",
        "\n",
        "    train_texts, test_texts, train_tags, test_tags, train_labels, test_labels = train_test_split(text, tags,\n",
        "                                                                                                 labels, test_size=.2)\n",
        "    \n",
        "    train_arg1, test_arg1, train_arg2, test_arg2 = train_test_split(args1, args2, test_size=.2)\n",
        "\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    train_ids, train_attn = tokenizeData_tag_sentence(tokenizer, train_texts, train_tags)\n",
        "    test_ids, test_attn = tokenizeData_tag_sentence(tokenizer, test_texts, test_tags)\n",
        "\n",
        "    trainFeatures = (train_ids, train_attn, train_labels)\n",
        "    testFeatures = (test_ids, test_attn)\n",
        "\n",
        "    trainDataLoader, testDataLoader = buildDataLoaders(8, trainFeatures, testFeatures)\n",
        "\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base').to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    train(3, 3, model, optimizer, trainDataLoader)\n",
        "\n",
        "    pred_labels = []\n",
        "    for text, tags in zip(test_texts, test_tags):\n",
        "        pred_labels.append(predict_tag_sentence(tokenizer, model, text, tags))\n",
        "\n",
        "    evaluate(pred_labels, test_labels)\n",
        "\n",
        "    pred_args = []\n",
        "    for arg1, arg2, label in zip(test_arg1, test_arg2, pred_labels):\n",
        "      pred_args.append({\"arg1\": arg1[1], \"arg2\": arg2[1], \"label\": label})\n",
        "    \n",
        "    with open('predictions.json', 'w') as f:\n",
        "      json.dump(pred_args, f, indent=4)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjWNUWOG7XIZ"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504,
          "referenced_widgets": [
            "269129c7a40c4f6e9f9def4a35caf940",
            "420d032cef094bc9b88cf9d298f44d22",
            "43b968cb6ec54b758349a34063b03afa",
            "ff29fa6eb4f44d68a62b7e3aa66ea7bd",
            "564c1743c73945d6a3e2ca796ef28f63",
            "9db23025980c436b94f1b8f81a38816d",
            "a18e81faa1b541c5874f08939dc51261",
            "d7d2b15048ec4f9daa44d1733b607bb9",
            "fdc66f4225d348b0936b723f3a369294",
            "6d0e9f30a12a40d6afac94afbf828954",
            "c740c9cfc6ba4eb8b7548df50869f9cf",
            "b622e9bfef6b49e4b9dd53e1d330fc62",
            "ba1eeff95bf141489ae0fe640ce78391",
            "303c8be56e184c6a9e6526636d3c3aac",
            "6a1d0a2761b84bacafd3a1ace881b929",
            "482b4b3d214e49f5b7404f298262962f"
          ]
        },
        "id": "Wlp34kMD7X95",
        "outputId": "6e3985d3-401f-4cd5-efa6-129056d2447e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(device)\n",
        "\n",
        "    file = input(\"Enter Dataset: \")\n",
        "\n",
        "    print(\"Select a model to train:\")\n",
        "    print(\"1.) BERT\")\n",
        "    print(\"2.) RoBERTa\")\n",
        "\n",
        "    model = int(input())\n",
        "\n",
        "    if model == 1:\n",
        "        print(\"Select a method:\")\n",
        "        print(\"1.) Entity Marker A\")\n",
        "        print(\"2.) Entity Marker B\")\n",
        "        print(\"3.) Entity Marker C\")\n",
        "        print(\"4.) Tag Sentence\")\n",
        "\n",
        "        method = int(input())\n",
        "\n",
        "        if method == 1:\n",
        "            bert_entity_marker_a(file)\n",
        "        elif method == 2:\n",
        "            bert_entity_marker_b(file)\n",
        "        elif method == 3:\n",
        "            bert_entity_marker_c(file)\n",
        "        else:\n",
        "            bert_tag_sentence(file)\n",
        "    \n",
        "\n",
        "    elif model == 2:\n",
        "        print(\"Select a method:\")\n",
        "        print(\"1.) Entity Marker A\")\n",
        "        print(\"2.) Entity Marker B\")\n",
        "        print(\"3.) Entity Marker C\")\n",
        "        print(\"4.) Tag Sentence\")\n",
        "\n",
        "        method = int(input())\n",
        "\n",
        "        if method == 1:\n",
        "            roberta_entity_marker_a(file)\n",
        "        elif method == 2:\n",
        "            roberta_entity_marker_b(file)\n",
        "        elif method == 3:\n",
        "            roberta_entity_marker_c(file)\n",
        "        else:\n",
        "            roberta_tag_sentence(file)\n",
        "\n",
        "    else:\n",
        "        print(\"Please select 1 or 2.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Enter Dataset: NYT.json\n",
            "Select a model to train:\n",
            "1.) BERT\n",
            "2.) RoBERTa\n",
            "2\n",
            "Select a method:\n",
            "1.) Entity Marker A\n",
            "2.) Entity Marker B\n",
            "3.) Entity Marker C\n",
            "4.) Tag Sentence\n",
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "269129c7a40c4f6e9f9def4a35caf940",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdc66f4225d348b0936b723f3a369294",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Training Epoch's: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:34<00:00, 71.51s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5630555555555555\n",
            "Standard Deviation: 0.044273935614427\n",
            "F1 Score: 0.5538263782545404\n",
            "Standard Deviation: 0.04353255955049212\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}